{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa14187-bcf4-4bb4-9b36-d060ed9d974b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --content CONTENT --styles STYLES [STYLES ...] [--style-blend STYLE_BLEND [STYLE_BLEND ...]] [--outdir OUTDIR]\n",
      "                             [--size SIZE] [--steps STEPS] [--content-weight CONTENT_WEIGHT] [--style-weight STYLE_WEIGHT] [--tv-weight TV_WEIGHT]\n",
      "                             [--lr LR] [--side-by-side] [--preserve-color]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --content, --styles\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# Artify Any Picture — Neural Style Transfer (VGG19)\n",
    "# ================================================\n",
    "# Features:\n",
    "#  - Content + one or many style images\n",
    "#  - Adjustable content/style weights, TV smoothing, steps, and size\n",
    "#  - Style BLENDING across multiple styles via weights\n",
    "#  - Optional color-preserving mode (approximate)\n",
    "#  - Saves stylized image + optional side-by-side comparison\n",
    "#\n",
    "# Usage (Windows examples):\n",
    "#   python artify_nst.py ^\n",
    "#     --content \"C:\\path\\to\\your\\photo.jpg\" ^\n",
    "#     --styles  \"C:\\Users\\sagni\\Downloads\\Artify AI\\archive\\VanGogh\\starry.jpg\" ^\n",
    "#     --outdir  \"C:\\Users\\sagni\\Downloads\\Artify AI\\outputs\" ^\n",
    "#     --size 768 --steps 500 --style-weight 1e6 --content-weight 1e5 --tv-weight 1e-5 --side-by-side\n",
    "#\n",
    "#   # Blend multiple styles (e.g., 70% VanGogh + 30% Hokusai):\n",
    "#   python artify_nst.py ^\n",
    "#     --content \"C:\\path\\photo.jpg\" ^\n",
    "#     --styles \"C:\\Artify AI\\archive\\VanGogh\\a.jpg\" \"C:\\Artify AI\\archive\\Hokusai\\b.jpg\" ^\n",
    "#     --style-blend 0.7 0.3 --steps 500\n",
    "#\n",
    "# Notes:\n",
    "#  - More steps -> better stylization (and slower). 300–700 is a good range.\n",
    "#  - If you see grain, raise --tv-weight (e.g., 1e-4).\n",
    "#  - Color-preserving approx: --preserve-color (helps keep content colors).\n",
    "# ================================================\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def load_image(path, max_size=None, device=\"cpu\"):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    if max_size is not None:\n",
    "        w, h = img.size\n",
    "        scale = max_size / max(w, h)\n",
    "        if scale < 1.0:\n",
    "            img = img.resize((int(w * scale), int(h * scale)), Image.LANCZOS)\n",
    "    return img\n",
    "\n",
    "def pil_to_tensor(img_pil, device):\n",
    "    tfm = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    t = tfm(img_pil).unsqueeze(0).to(device)\n",
    "    return t\n",
    "\n",
    "def tensor_to_pil(tensor):\n",
    "    tensor = tensor.detach().cpu().clone().squeeze(0)\n",
    "    unnorm = T.Normalize(mean=[-m/s for m, s in zip([0.485,0.456,0.406],[0.229,0.224,0.225])],\n",
    "                         std=[1/s for s in [0.229,0.224,0.225]])\n",
    "    img = unnorm(tensor).clamp(0, 1)\n",
    "    return T.ToPILImage()(img)\n",
    "\n",
    "def match_color_simple(content_pil, stylized_pil):\n",
    "    \"\"\"\n",
    "    Quick color transfer: match mean/std in LAB space (approximate).\n",
    "    Keeps original content color mood while using style textures.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import cv2\n",
    "        c = cv2.cvtColor(np.array(content_pil), cv2.COLOR_RGB2LAB).astype(np.float32)\n",
    "        s = cv2.cvtColor(np.array(stylized_pil), cv2.COLOR_RGB2LAB).astype(np.float32)\n",
    "        c_mean, c_std = c.mean(axis=(0,1)), c.std(axis=(0,1)) + 1e-6\n",
    "        s_mean, s_std = s.mean(axis=(0,1)), s.std(axis=(0,1)) + 1e-6\n",
    "        matched = (s - s_mean) / s_std * c_std + c_mean\n",
    "        matched = np.clip(matched, 0, 255).astype(np.uint8)\n",
    "        out = cv2.cvtColor(matched, cv2.COLOR_LAB2RGB)\n",
    "        return Image.fromarray(out)\n",
    "    except Exception:\n",
    "        # If OpenCV not available, just return stylized\n",
    "        return stylized_pil\n",
    "\n",
    "# ---------------------------\n",
    "# VGG Feature Extractor\n",
    "# ---------------------------\n",
    "class VGGFeatures(nn.Module):\n",
    "    def __init__(self, layers_content, layers_style):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
    "        # Freeze weights\n",
    "        for p in vgg.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        self.vgg = vgg.eval()\n",
    "        self.layers_content = layers_content\n",
    "        self.layers_style = layers_style\n",
    "\n",
    "    def forward(self, x):\n",
    "        content_feats = {}\n",
    "        style_feats = {}\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.layers_content:\n",
    "                content_feats[name] = x\n",
    "            if name in self.layers_style:\n",
    "                # Gram for style\n",
    "                b, c, h, w = x.shape\n",
    "                feat = x.view(b, c, h*w)\n",
    "                gram = torch.bmm(feat, feat.transpose(1,2)) / (c*h*w)\n",
    "                style_feats[name] = gram\n",
    "            if len(content_feats) == len(self.layers_content) and len(style_feats) == len(self.layers_style):\n",
    "                # early break possible\n",
    "                pass\n",
    "        return content_feats, style_feats\n",
    "\n",
    "# ---------------------------\n",
    "# NST Optimize\n",
    "# ---------------------------\n",
    "def stylize(content_img, style_imgs, blend_weights, size, steps,\n",
    "            content_weight, style_weight, tv_weight, lr, device):\n",
    "    # Load & preprocess\n",
    "    content_pil = load_image(content_img, max_size=size, device=device)\n",
    "    content_t = pil_to_tensor(content_pil, device)\n",
    "\n",
    "    style_tensors = []\n",
    "    for sp in style_imgs:\n",
    "        s_pil = load_image(sp, max_size=size, device=device)\n",
    "        style_tensors.append(pil_to_tensor(s_pil, device))\n",
    "    if blend_weights is None or len(blend_weights) != len(style_tensors):\n",
    "        blend_weights = [1.0/len(style_tensors)] * len(style_tensors)\n",
    "    # normalize weights\n",
    "    S = sum(blend_weights)\n",
    "    blend_weights = [w / S for w in blend_weights]\n",
    "\n",
    "    # Feature layers (commonly used selections)\n",
    "    layers_content = {\"21\"}  # relu4_2\n",
    "    layers_style   = {\"0\",\"5\",\"10\",\"19\",\"28\"}  # relu1_1, relu2_1, relu3_1, relu4_1, relu5_1\n",
    "\n",
    "    extractor = VGGFeatures(layers_content, layers_style).to(device)\n",
    "\n",
    "    # Target features\n",
    "    with torch.no_grad():\n",
    "        cF, _ = extractor(content_t)\n",
    "        # Weighted style grams\n",
    "        target_style = {k: 0 for k in layers_style}\n",
    "        for s_t, w in zip(style_tensors, blend_weights):\n",
    "            _, sF = extractor(s_t)\n",
    "            for k in layers_style:\n",
    "                target_style[k] = target_style[k] + w * sF[k]\n",
    "\n",
    "    # Initialize output image (start from content)\n",
    "    x = content_t.clone().requires_grad_(True)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam([x], lr=lr)\n",
    "\n",
    "    # TV Loss (smoothness)\n",
    "    def tv_loss(x):\n",
    "        a = torch.mean(torch.abs(x[:, :, :, :-1] - x[:, :, :, 1:]))\n",
    "        b = torch.mean(torch.abs(x[:, :, :-1, :] - x[:, :, 1:, :]))\n",
    "        return a + b\n",
    "\n",
    "    # Optimize\n",
    "    for i in range(1, steps+1):\n",
    "        optimizer.zero_grad()\n",
    "        cFx, sFx = extractor(x)\n",
    "\n",
    "        # Content loss\n",
    "        c_loss = 0.0\n",
    "        for k in layers_content:\n",
    "            c_loss = c_loss + torch.mean((cFx[k] - cF[k])**2)\n",
    "\n",
    "        # Style loss\n",
    "        s_loss = 0.0\n",
    "        for k in layers_style:\n",
    "            s_loss = s_loss + torch.mean((sFx[k] - target_style[k])**2)\n",
    "\n",
    "        # Total variation\n",
    "        t_loss = tv_loss(x)\n",
    "\n",
    "        loss = content_weight * c_loss + style_weight * s_loss + tv_weight * t_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % max(1, steps//10) == 0 or i == 1:\n",
    "            print(f\"[{i:4d}/{steps}] total={float(loss):.4e}  \"\n",
    "                  f\"C={float(c_loss):.4e} S={float(s_loss):.4e} TV={float(t_loss):.4e}\")\n",
    "\n",
    "    out_pil = tensor_to_pil(x)\n",
    "    return content_pil, out_pil\n",
    "\n",
    "# ---------------------------\n",
    "# CLI\n",
    "# ---------------------------\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"Artify any picture via Neural Style Transfer (VGG19).\")\n",
    "    p.add_argument(\"--content\", required=True, type=str, help=\"Path to content image (your photo).\")\n",
    "    p.add_argument(\"--styles\", required=True, nargs=\"+\", type=str, help=\"One or more style image paths.\")\n",
    "    p.add_argument(\"--style-blend\", nargs=\"+\", type=float, default=None,\n",
    "                   help=\"Weights for each style (sum auto-normalized).\")\n",
    "    p.add_argument(\"--outdir\", type=str, default=\"./artify_outputs\", help=\"Directory to save outputs.\")\n",
    "    p.add_argument(\"--size\", type=int, default=768, help=\"Max size (longest side).\")\n",
    "    p.add_argument(\"--steps\", type=int, default=400, help=\"Optimization steps (300–700 good).\")\n",
    "    p.add_argument(\"--content-weight\", type=float, default=1e5, help=\"Content weight.\")\n",
    "    p.add_argument(\"--style-weight\", type=float, default=1e6, help=\"Style weight.\")\n",
    "    p.add_argument(\"--tv-weight\", type=float, default=1e-5, help=\"Total variation (smoothing) weight.\")\n",
    "    p.add_argument(\"--lr\", type=float, default=0.02, help=\"Adam learning rate.\")\n",
    "    p.add_argument(\"--side-by-side\", action=\"store_true\", help=\"Save a side-by-side comparison image.\")\n",
    "    p.add_argument(\"--preserve-color\", action=\"store_true\", help=\"Approx color preservation using LAB stats.\")\n",
    "    args = p.parse_args()\n",
    "\n",
    "    outdir = Path(args.outdir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "\n",
    "    content_pil, stylized_pil = stylize(\n",
    "        content_img=args.content,\n",
    "        style_imgs=args.styles,\n",
    "        blend_weights=args.style_blend,\n",
    "        size=args.size,\n",
    "        steps=args.steps,\n",
    "        content_weight=args.content_weight,\n",
    "        style_weight=args.style_weight,\n",
    "        tv_weight=args.tv_weight,\n",
    "        lr=args.lr,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    if args.preserve_color:\n",
    "        stylized_pil = match_color_simple(content_pil, stylized_pil)\n",
    "\n",
    "    # Save outputs\n",
    "    c_name = Path(args.content).stem\n",
    "    if len(args.styles) == 1:\n",
    "        s_name = Path(args.styles[0]).stem\n",
    "    else:\n",
    "        s_name = \"blend_\" + \"_\".join([Path(s).stem for s in args.styles])[:80]\n",
    "\n",
    "    out_img = outdir / f\"artified_{c_name}_with_{s_name}.png\"\n",
    "    stylized_pil.save(out_img)\n",
    "    print(f\"[SAVE] Stylized -> {out_img}\")\n",
    "\n",
    "    if args.side_by_side:\n",
    "        w1, h1 = content_pil.size\n",
    "        w2, h2 = stylized_pil.size\n",
    "        h = max(h1, h2)\n",
    "        s1 = content_pil.resize((w1, h), Image.LANCZOS) if h1 != h else content_pil\n",
    "        s2 = stylized_pil.resize((w2, h), Image.LANCZOS) if h2 != h else stylized_pil\n",
    "        side = Image.new(\"RGB\", (s1.width + s2.width, h), (255, 255, 255))\n",
    "        side.paste(s1, (0, 0))\n",
    "        side.paste(s2, (s1.width, 0))\n",
    "        out_side = outdir / f\"side_by_side_{c_name}_with_{s_name}.png\"\n",
    "        side.save(out_side)\n",
    "        print(f\"[SAVE] Side-by-side -> {out_side}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11a4b7-8132-4c8e-a042-1fd7acc8ff0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
